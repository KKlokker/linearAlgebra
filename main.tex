\documentclass[12pt, a4paper]{article}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{tikz-network}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\title{Linear algebra}
\date{2022}
\author{Kristoffer Klokker}

\usepackage{xcolor,listings}
\usepackage{textcomp}
\usepackage{color}
\usepackage{listings}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{HTML}{C42043}
\definecolor{backcolour}{HTML}{F2F2F2}
\definecolor{bookColor}{cmyk}{0,0,0,0.90}  
\color{bookColor}

\lstset{upquote=true}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codepurple},
    numberstyle=\numberstyle,
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=3,
}
\lstset{style=mystyle}
\usepackage{zref-base}

\makeatletter
\newcounter{mylstlisting}
\newcounter{mylstlines}
\lst@AddToHook{PreSet}{%
  \stepcounter{mylstlisting}%
  \ifnum\mylstlines=1\relax
    \lstset{numbers=none}
  \else
    \lstset{numbers=left}
  \fi
  \setcounter{mylstlines}{0}%
}
\lst@AddToHook{EveryPar}{%
  \stepcounter{mylstlines}%
}
\lst@AddToHook{ExitVars}{%
  \begingroup
    \zref@wrapper@immediate{%
      \zref@setcurrent{default}{\the\value{mylstlines}}%
      \zref@labelbyprops{mylstlines\the\value{mylstlisting}}{default}%
    }%
  \endgroup
}

% \mylstlines print number of lines inside listing caption
\newcommand*{\mylstlines}{%
  \zref@extractdefault{mylstlines\the\value{mylstlisting}}{default}{0}%
}
\makeatother


\newcommand\numberstyle[1]{%
    \footnotesize
    \color{codegray}%
    \ttfamily
    \ifnum#1<10 0\fi#1 |%
}


\begin{document}
	\maketitle
	\clearpage
	\tableofcontents
	\clearpage
	\section{Systems of linear Systems of Linear Equations}
		A system of linear equations are multiple equations containing unknows which are shared. Ex.
		\begin{alignat*}{4}
		   2x & {}+{} &  y & {}+{} & 3z & {}={} & 10 \\
		    x & {}+{} &  y & {}+{} &  z & {}={} &  6 \\
		    x & {}+{} & 3y & {}+{} & 2z & {}={} & 13
		\end{alignat*}
		The same system may be wirtten in form of a matrix\\
		$$\begin{bmatrix}
			2 & 1 & 3 & 10\\
			1 & 1 & 1 & 6\\
			1 & 3 & 2 & 13
		\end{bmatrix}$$
		When solving a system their may be
		\begin{itemize}
			\item No solutions - No possible value can be assigned to the variable such all equation are true (inconsistent)
			\item One solution - A combination of values can be assigned to make every equation true (consistent)
			\item Infinite solutions - One or more unknows may have an infinite amount of possible assignable values (consistent)
		\end{itemize}
		By transforming a system of equations to a matrice, the following row operations can be performed:
		\begin{itemize}
			\item Multiply a row through by a nonzero constant.
			\item Interchange two rows.
			\item Add a constant times one row to another.
		\end{itemize}
		\subsection{Gauss-Jordan Elimination}
			Solving a system of equation in a matrice, can be done such the matrice has the following requirements
			\begin{itemize}
				\item If the row contain nothing but zeroes the first number should be a 1, called the leading 1.
				\item If a row is made of nothing but zeroes it should be grouped at the bottom
				\item In two rows the top row should contain a leading 1 further to the left than the bottom
				\item Each column which contains a leading 1, every number in the same column below should be 0
			\end{itemize}
			This form is called row echelon form.\\
			The solution may also be written as:
			$$\{(x_1=4,x_2=6,x_3=t,x_4=v,x_5=1)|t\in \mathbb{R} \}$$
			Where variables means they can be any possible asignment or a function with given restrictions, called free variables.\\
			In case of the leading 1 column is zero both above and underneath the matrice is in reduced echlon form.\\[4mm]
			To make a matrice into echoleon form the following algorithm can be used:\\
			\begin{enumerate}
				\item Locate the leftmost column that does not consist entruely of zeros and exhange it to the top
				\item Multiply the top row by $\frac{1}{a}$ where $a$ is the leading number in the row
				\item subtract top row from every other below row, such the top row is the only non zero value in the column
				\item Repeat 2 and 3 but ignore the top row and let the second top row be the top
			\end{enumerate}
			To reduce the echloen form, from the bottom the bottom row is added to the above rows until the leading 1 is the only in the column. This is repeated until the top is reached.\\
			A homogeneous linear system are systems which all constant (right part of equal) are 0 and the trivial solution (all variables are assigned 0) are a possible solution.\\
			A free variable is the term for a variable which can be assigned multiple values. The number of free variables wil lbe equal to the number of variables minus zero rows.\\
			In a homogenous linear system if the number of unknows exceed the number of equation there will be an infinite amount of solutions.\\
			Back substitution is a method taking the echoloen form and starting from the bottom isolating the leading variable and substituting it upwards.\\
			A echoloen form is not unique to a system but a reduced echoloen form is unique and the number of zero rows will be unique.
		\subsection{Matrices and Matrix operations}
			A matrix is a rectangular array of numbers called entries.\\
			The size of a matrix is written as rows x columns\\
			A single row matrix is alled a row vector and single column is called column vector.\\
			Standard basis  is the set of column vectors in a identity matrix.\\
			Variables in matrices are called scalars and unless stated is in the realm of real numbers.\\
			When refering to a number in the matrix A it will be $a_{row\;column}$ and the value can be found by $(A)_{row\;column}$\\
			A matrix scalar matrix can be written as $A=[a_{ij}]_{m\times n}$ the $m\times n$ is optional if the size matters.\\
			\subsubsection{Matrix operations}
				Addition/subtraction - every number in corresponding entries are added/subtracted.\\
				Addtion and subtraction can not be done on two different sized matrices.\\[4mm]
				Multiplying a matrix with a scalar, is done by multiplying the scalar on every entry.\\
				For multiplying two matrices an entry is found by $a_{ij}=\sum\limits_{t=0}^n b_{i(j+t)}\cdot c_{(i+t)j}$ where $n$ is the size of row size of matrix $b$ and column size of matrix $c$, if this value does not match the two matrices can not be multiplied.\\
				The reuslting product matrix of $a = b \times c$ will have the size rows = c columns and columns = b rows\\[4mm]
				Partioning a matrix is the act of splitting it into a smaller valid matrices. This can be used if only certain entries are wanted for a matrix multiplication\\
				Linear combinations are an array of matrices (length $n$) of the same size matrix $A$ and an array of scalars (length $n$) $c$ computed as $\sum\limits_{i=o}^n A_i\cdot c_i$\\
				Multiplying a vector row and a matrix (with approprate lengths) can therefore be expressed as an linear combination.\\
				Column row expansion is the act of splitting two matrices being multiplied into rows and columns, and then taking the sum of each product of row and column, and thus getting the muiltiplied matrix.\\[4mm]
				Matrix form of linear system is the act of expressing a linear system in the form of matrix $A$ containing all coefficients, matrix $X$ containing a column vector of all unknown variables, and matrix $B$ which is a column vector of all results of the linear system. in this way the linear system can be expressed as $AX=B$\\
				Tranposing a matrix is flipping column and rows such in matrix $A$ the entry $a_{ij}$ is now entry $a_{ji}$ in the matrix $A^T$.\\
				A trace operation of matrix $A$ is taking the addition of the diagonal line called $tr(A)$, can only be performed on square matrices.\\
				The trace of a square matrix is defined as $tr(A)=a_{11}+a_{22}+...+a_{nn}$ or $tr(A)=\sum\limits_{i=0}^n a_{ii}$\\
				For the trace the following is true
				\begin{itemize}
					\item $tr(A^T)=tr(A)$
					\item $tr(A\pm B)=tr(A)\pm tr(B)$
					\item $tr(cA)=c\cdot tr(A)$
					\item $tr(AB)=tr(BA)$
				\end{itemize}
				The matrix polynomials are a way of describing multiple equations in matrix form by\\
				If $A$ contains all the coefficients, $X$ contains all variables in a column vector, and $B$ contains all constant the equations are equal to in a column vector.\\
				Then the equations can be described as  $A\cdot X=B$
		\subsection{Inverses}
			For matrix operations are the following true, if the size allows for it
			\begin{itemize}
				\item Commutative law for matrix addition - $A + B = B + A$
				\item Associative law for matrix addition - $A + ( B + C) = (A+B)+C$
				\item Associative law for matrix multiplication - $A(BC)=(AB)C$\\
				\item Left and Right distributive law - $A(B+C)=AB+AC$
			\end{itemize}
			Zero matrices are matrices consisting of only zeros and are denoted by $0_{m\times n}$\\
			Identity matrix $I$ is the matrix containing 1's on the diagonal, and has the proberty that if multiplies by $A$ the product is $A$.\\
			The inverse matrix $A^{-1}$ is the matrix $B$ to the square $A$ if $AB=I$, and $A$ is said to be invertible. If $B$ does not exist $A$ is singular\\
			Only one inverse exist to a matrix.\\
			For two inverse matrices is the following true $B^{-1}A^{-1}=(AB)^{-1}$.\\
			Powers if a matrix is defined as expected, and for $A^0=I$ and if $A$ is invertible then $A^{-n}=(A^{-1})^n$\\
			$A^n$ is invertible and $(A^n)^{-1}=A^{-n}=(A^{-1})^n$\\
			Matrix pylonomials are inserting a matrix into a polynomial function.\\
			Transpose if the interchangning of rows in a matrix, and are expressed as $(A^T)^{-1}=(A^{-1})^T$.\\
			The transposed matrix has the follwoing properties:
			\begin{itemize}
				\item $(A^T)^T=A$
				\item $(A\pm B)^T=A^T\pm B^T$ 
				\item $(cA)^T=cA^T$
				\item $(AB)^T=B^TA^T$
			\end{itemize}
		\subsection{Determinants by cofactor expansion}
			Minor of entry $a_{ij}$ is denoted $M_{ij}$ and is defined as the determinant of the submatrix that remains when i row and j column is deleted.\\
			The cofactor $C_{ij}$ of $M_{ij}$ is defined as, if $i+j$ is even then $1^{i+j}M_{ij}$ and $(-1)^{i+j}M_{ij}$ if odd\\
			The determinant of an $n\times n$ matrix can be found either by a row or column in the following way:
			$$det(A)=a_{1j}C_{1j}+a_{2j}C_{2j}+...+a_{nj}C_{nj}$$
			This is via the column and by switching $i$ and $j$ it is by the row.\\
			Therefore making the definition recursive.\\
			If we make the function $f(i,j)=1 - ((i+j) \% 2 ) \cdot 2$ such it will be $-1$ when the sum of $i$ and $j$ is odd and $1$ when the sum is even.
			$$det(A)=a_{1j}f(1,j)^{1+j}M_{1j}+a_{2j}f(2,j)^{2+j}M_{2j}+...+a_{nj}f(n,j)^{n+j}M_{nj}$$
			if we also denote the function $g(A,i,j)$ to be a function which removes row $i$ and $j$\\
			Then we can make a true recursive definition.\\
			$$det(A)=a_{1j}f(1,j)^{1+j}det(g(A,1,j)+a_{2j}f(2,j)^{2+j}det(g(A,2,j)+...+a_{nj}f(n,j)^{n+j}det(g(A,n,j)$$
			Where for the 1x1 matrix $det([a_{11}])=a_{11}$
			If $A$ is triangular then $det(A)$ is equal to the product of the main diagonal.\\[4mm]
			It can also be noted that determination of the zero matrix is zero\\
			And for the transpose $det(A)=det(A^T)$\\[4mm]
			To make life easier row operation can be done such less operations are needed to find the determinant.\\
			When performning row operation the following is the for the $A$ matrix with size $n\times n$
			\begin{itemize}
				\item If $B$ is the result of a row or column in $A$ being multipled by a constant $k$ then $det(B)=k\cdot det(A)$
				\item if $B$ is the result of two rows or two columns being interchanged then $det(B)=-det(A)$
				\item if $B$ is the result of one row being multiplied and added to another row then $det(B)=det(A)$
			\end{itemize}
			For the special case where $A=I_n$ and $E$ is the result of an row operation on $A$ the follwing applies
			\begin{itemize}
				\item If $E$ is the result of multiplying a row with the constant $k$ which is not 0 then $det(E)=k$
				\item If $E$ is the result of interchaning two rows then $det(E)=-1$
				\item If $E$ is the result of adding a constant to a row and adding to another then $det(E)=1$
			\end{itemize}
			If $A$ has a row/column which is proportional to another then $det(A)=0$\\
			Reducing a matrix the echeloen, in doing so instead of reducing the a row by 1/ the leading value, the leading value is taken to the side, then the product of every value taken to the side will be equal to the determinant.\\
		\subsection{Properties of determinants}
			If $A$ is a $n\times n$ matrix and $C_{ij}$ is the cofactor of $A_{ij}$, then the matrix is called matrix of cofactors from $A$\\
			The transpose of the matrix of cofactors from $A$ is called adjoint of $A$
			\begin{itemize}
				\item $det(kA)=k^ndet(A)$
				\item $det(C)=det(A)+det(B)$  If A and B only differ in one row/column and C is equal to A and the the differ row/column sum
				\item $det(AB)=det(A)det(B)$ if both $A$ and $B$ are $n\times n$ matrices
				\item $A$ is only invertible if $det(A)\neq 0$
				\item If $A$ is invertible then $det(A^{-1})=\frac{1}{det(A)}$
				\item If $A$ is invertible then $A^{-1}=\frac{1}{det(A)}adj(A)$
			\end{itemize}
			In the system of linear equation $Ax=b$ then the solutions can be found by
			$$x_n=\frac{det(A_n)}{det(A)}$$
		\subsection{Matrix transformation}
			The matrix transformation is function $T_A(x)=A\cdot x$, which transforms the axis linearly, such origo stays in the same place.\\
			The function then takes a vector of length $n$ and multiply it / transforms it with a matrix of size $n\times n$\\
			The transformation which has the follwing proberties:
			\begin{itemize}
				\item $T_A(0)=0$ Origin stays in place
				\item $T_A(k\cdot x) = k\cdot T_A(x)$ 
				\item $T_A(x+y)=T_A(x)+T_A(y)$
				\item $T_A(x-y)=T_A(x)-T_A(y)$
			\end{itemize}
			To think of the matrix $A$ which the transformation use, it can be though of in the coordination system x,y,z $A$ would then be a 3x3 matrix.\\
			Each column then represent the standard basis for the axis. \\
			Such for the Cartesian (standard) coordination system, the matrix would be a 3x3 identity matrix.\\
			Generally speaking the identity matrix is also called to standard matrix\\[4mm]
			The transformation may also be composited together like $(T_B\cdot T_A)(x)=T_B(T_A(X))$\\
			Likewise multiplication the transformation is not commutative $T_A\cdot T_B\neq T_B \cdot T_A$\\
			The exception to this is rotation and reflection transformations.\\
			If $A$ is invertible then $T_A:R^n\rightarrow R^n$ $R^n$ is also invertible.\\
			This is written as $T_A^{-1}=T_{A^{-1}}$\\
			A linear transformation is when $T:\mathbb{R}^N\rightarrow\mathbb{R}^M$ statisfies the follwing linear conditions
			\begin{itemize}
				\item $T(k\cdot x) = k\cdot T(x)$
				\item $T(x+y)=T(x)+T(y)$
			\end{itemize}
		\subsection{Projections}
			A projection is a transformation to a lower dimension system.\\
			An orthogonal projections is a projection on a line, plane or the appropriate amount of dimensions.\\
	\section{Vector spaces}
		If $V$ defines all objects which are non empty then
		\begin{itemize}
			\item $u,v\in V\rightarrow v+u\in V$
			\item $u+v=v+u$
			\item $u+(v+w)=(u+v)+w$
			\item The zero vector exist called $0$ with the property $v+0=v$
			\item For each $u$ in $V$ exists a $-u$ such $u+(-u)=0$
			\item If $k$ is a scalar and $u$ is in $V$ then $uk$ is in $V$
			\item  $k(u+v)=ku+kv$
			\item $k(mu)=(km)u$
			\item $1u=u$
		\end{itemize}
		If an object satisfies all these axioms then we can classify a set of objects as being in the  vector space.\\
		Therefore $\mathbb{R}$ is in the vector space, $\mathbb{R}^n$ being the vector of real number length $n$ is in the vector space.\\
		Likewise $\mathbb{R}^{n\cdot m}$ matrix is in the vector space and even functions like the polynomial functions are in the vector space.\\
		\subsection{Subspaces}
			A subspace $W$ of $V$ is a subspace if $u$ and $v$ are in $W$ and $u+v$ is in $W$.\\
			And if $k$ is a scalar and $u$ is in $W$ then $ku$ is in $W$\\ 
			
			
			
			
			
				
\end{document}

